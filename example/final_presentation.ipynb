{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b975cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytest\n",
    "\n",
    "import copy as copy\n",
    "import json\n",
    "import networkx as nx\n",
    "from power_grid_model.utils import json_deserialize, json_serialize\n",
    "import power_system_simulation.graph_processing as pss\n",
    "import power_system_simulation.pgm_processing as pgm_p\n",
    "import power_system_simulation.optimization as psso\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pytest\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "# Some graphics processing\n",
    "import xml.etree.ElementTree as ET\n",
    "from IPython.display import SVG, display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import power_system_simulation.graph_processing as pss\n",
    "import power_system_simulation.optimization as psso\n",
    "from power_grid_model.utils import json_deserialize, json_serialize\n",
    "\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b52833-9e5e-40c8-a268-43a2bc0c35a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph processing\n",
    "    \n",
    "red = '#f54040'\n",
    "green = '#86fc53'\n",
    "blue = '#1cd5ff'\n",
    "\n",
    "svg_content = ''\n",
    "\n",
    "def update_style(svg_code, element_id, new_style):\n",
    "    pattern = f'(<[^>]+id=\"{element_id}\"[^>]+style=\")[^\"]*(\")'\n",
    "    replacement = f''\n",
    "    if \"e\" in element_id:\n",
    "        replacement = f'\\\\1stroke:{new_style}\\\\2'\n",
    "    elif \"v\" in element_id:\n",
    "        replacement = f'\\\\1fill:{new_style}\\\\2'\n",
    "        \n",
    "    updated_svg_code = re.sub(pattern, replacement, svg_code)\n",
    "    updated_svg_code = re.sub(f'(<[^>]+id=\"t{element_id}\"[^>]+style=\")[^\"]*(\")', f'\\\\1fill:{new_style}\\\\2', updated_svg_code)\n",
    "        \n",
    "    return updated_svg_code\n",
    "\n",
    "def update():\n",
    "    global svg_content\n",
    "    style = \"<style>svg{width:50px !important;height:50px !important;</style>\"\n",
    "    display(SVG(svg_content), clear=True)\n",
    "\n",
    "def blink_edges(edge_ids, delay, times=5, color='#f54040'):\n",
    "    for i in range(times):\n",
    "        svg_content_a = svg_content\n",
    "        svg_content_b = svg_content\n",
    "        \n",
    "        for edge in edge_ids:\n",
    "            svg_content_a=update_style(svg_content_a, f'e{edge}', color)\n",
    "        display(SVG(svg_content_a), clear=True)\n",
    "        time.sleep(delay)\n",
    "        for edge in edge_ids:\n",
    "            svg_content_b=update_style(svg_content_b, f'e{edge}', 'gray')\n",
    "        display(SVG(svg_content_b), clear=True)\n",
    "        time.sleep(delay)\n",
    "        \n",
    "def blink_vertices(vertex_ids, delay, times=5, color='#f54040'):\n",
    "    for i in range(times):\n",
    "        svg_content_a = svg_content\n",
    "        svg_content_b = svg_content\n",
    "        \n",
    "        for vertex in vertex_ids:\n",
    "            svg_content_a=update_style(svg_content_a, f'v{vertex}', color)\n",
    "        display(SVG(svg_content_a), clear=True)\n",
    "        time.sleep(delay)\n",
    "        for vertex in vertex_ids:\n",
    "            svg_content_b=update_style(svg_content_b, f'v{vertex}', 'white')\n",
    "        display(SVG(svg_content_b), clear=True)\n",
    "        time.sleep(delay)\n",
    "        \n",
    "def highlight_disabled_edges(gp):\n",
    "    global svg_content\n",
    "    for edge_index in range(len(gp.edge_ids)):\n",
    "        if gp.edge_enabled[edge_index] == False:\n",
    "            svg_content = update_style(svg_content, f'e{gp.edge_ids[edge_index]}', '#f54040')\n",
    "            update()\n",
    "            \n",
    "def visualizer_init(gp):\n",
    "    global svg_content\n",
    "    \n",
    "    with open('Asset 1.svg', 'r') as file:\n",
    "        svg_content = file.read()\n",
    "    \n",
    "    for edge_index in range(len(gp.edge_ids)):\n",
    "        if gp.edge_enabled[edge_index] == False:\n",
    "            svg_content = update_style(svg_content, f'e{gp.edge_ids[edge_index]}', '#303030')\n",
    "    update()\n",
    "            \n",
    "def highlight_edge(edge_id, color='#86fc53'):\n",
    "    global svg_content\n",
    "    svg_content = update_style(svg_content, f'e{edge_id}', color)\n",
    "    update()\n",
    "    \n",
    "def highlight_vertex(vertex_id, color='#86fc53'):\n",
    "    global svg_content\n",
    "    svg_content = update_style(svg_content, f'v{vertex_id}', color)\n",
    "    update()\n",
    "    \n",
    "def visualizer_alternative_edge(gp, edge_id):\n",
    "    highlight_edge(edge_id, red)\n",
    "    alternative_edges = gp.find_alternative_edges(edge_id)\n",
    "    for edge in alternative_edges:\n",
    "        highlight_edge(edge, color=green)\n",
    "    \n",
    "def visualizer_downstream_vertices(gp, edge_id):\n",
    "    highlight_edge(edge_id, red)\n",
    "    downstream_vertices = gp.find_downstream_vertices(edge_id)\n",
    "    for vertex in downstream_vertices:\n",
    "        highlight_vertex(vertex, color=green)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d6296d-9187-4737-ab9b-b73bc0d521bc",
   "metadata": {},
   "source": [
    "# **Power Systems Calculation Package: Myriad**\n",
    "\n",
    "Introduction\n",
    "- Team Myriad\n",
    "- Energy Transition\n",
    "    - A future with less polution\n",
    "    - Renewable energy sources\n",
    "    - Electrical vehicles\n",
    "    - Eeat pumps\n",
    "- Uncertainties in the grid\n",
    "    - Applying statistical methods and relevant data science techniques in the design and operation of the grid.\n",
    "- Multidiciplinary profiles:\n",
    "    - Electrical power engineering, data science, and scientific software engineering \n",
    "- Assignments 1, 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cb9277",
   "metadata": {},
   "source": [
    "# **General Overview**\n",
    "\n",
    "This package includes multiple types of functions, some of the important ones are:\n",
    "- EV Penetration calculations\n",
    "- Optimal tap position calculation\n",
    "- Alternative edge searching\n",
    "- Testing\n",
    "- Teamwork\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d652f",
   "metadata": {},
   "source": [
    "# **EV Penetration**\n",
    "\n",
    "The main goal of this function is to randomly assign EV (electrical vehicle) charging profiles to houses that have a sym_load,\n",
    "given a user-provided input for the penetration level. \n",
    "\n",
    "This can be done step-by-step, beginning with the conversion of the input data. This is done because the Downstream Vertices function from assignment 1 is also used for this function:\n",
    "\n",
    "```python\n",
    "# Make Graphprocessing instance to draw the graph\n",
    "vertex_ids = [node[0] for node in self.pgm_input[\"node\"]]\n",
    "edge_ids = [edge[0] for edge in self.pgm_input[\"line\"]]\n",
    "edge_vertex_id_pairs = [(edge[1], edge[2]) for edge in self.pgm_input[\"line\"]]\n",
    "edge_enabled = [(edge[3] == 1 and edge[4] == 1) for edge in self.pgm_input[\"line\"]]\n",
    "source_vertex_id = self.pgm_input[\"source\"][0][1]\n",
    "\n",
    "# Pretend all transformers are short circuits, so that GraphProcessor can use it\n",
    "for transformer in self.pgm_input[\"transformer\"]:\n",
    "    edge_vertex_id_pairs.append((transformer[1], transformer[2]))\n",
    "    edge_enabled.append(True)\n",
    "    edge_ids.append(transformer[0])\n",
    "gp = pss.GraphProcessor(\n",
    "    vertex_ids, edge_ids, edge_vertex_id_pairs, edge_enabled, source_vertex_id\n",
    ")\n",
    "```\n",
    "Now that we have converted the given input data, we can initialize a graph processor object:\n",
    "\n",
    "```python \n",
    "gp = pss.GraphProcessor(\n",
    "    vertex_ids, edge_ids, edge_vertex_id_pairs, edge_enabled, source_vertex_id\n",
    ")\n",
    "```\n",
    "Now the Downstream Vertices function from assignment 1 can be used to find which nodes belong to which LV feeders:\n",
    "\n",
    "```python\n",
    "# See which feeder has which nodes\n",
    "        for feeder_id in self.meta_data[\"lv_feeders\"]:\n",
    "            feeders.append(feeder_id)\n",
    "            feeder_nodes[feeder_id] = gp.find_downstream_vertices(feeder_id)\n",
    "```\n",
    " \n",
    "After finding which nodes belong to which LV feeders, the calculation can be shown in a couple of points:\n",
    "- Calculating the amount of EV-houses per LV feeder\n",
    "    - The amount of EV-houses per LV feeder is calculated using: ``math.floor(penetration_level * total_houses / total_feeders)`` in order to round down to the nearest intiger.\n",
    "- Finding which of the nodes are houses for each LV feeder.\n",
    "    - This is done by checking which node ID's match the ID's in sym_load from the input data and then assigning only those to their corresponding LV feeder.\n",
    "- Randomly choose houses that will get an EV charger.\n",
    "    - Using the random.sample function to assign multiple random profiles to houses, making sure that there are no repetitives.\n",
    "- Assign random EV charging profiles to the chosen houses \n",
    "    - This time using the random.shuffle with a for loop was needed to randomly choose a profile for each house without repetitives.\n",
    "- Assign sym_load nodes to input_network_data.json IDs\n",
    "- Modify the active and reactive load profile according to the EV house profile\n",
    "- Run a time-series power flow and return the two aggregation tables\n",
    "    - Using the functions create_update_model, run_batch_process, and get_aggregate_results from Assignment 2 to print the tables as Dataframes.\n",
    "\n",
    "## Downstream Vertices Algorithm Steps\n",
    "\n",
    "- Remove the input edge from the id_pairs list\n",
    "- Find the islands\n",
    "- Check which remaining islands contain the source vertex, the other island is our output\n",
    "\n",
    "Or in code form:\n",
    "```python\n",
    "# Create two islands by seperating the graph at the input edge\n",
    "edge_list.remove(input_edge)\n",
    "\n",
    "islands = calculate_islands()\n",
    "\n",
    "for island in islands:\n",
    "    if not island.contains(source_vertex):\n",
    "        return island\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42f97cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" style=\"background:transparent\" viewBox=\"0 0 800 228.111\">\n",
       "\t<defs>\n",
       "\t\t<style>rect{fill:white} .cls-1,.cls-2{fill:none;stroke:gray;stroke-miterlimit:10;}.cls-2{fill-rule:evenodd;}.cls-3,.cls-5{font-size:12px;font-family:ArialMT, Arial;}.cls-4{letter-spacing:-0.074em;} .cls-3{ fill:white}</style>\n",
       "\t</defs>\n",
       "\t<g id=\"Layer_2\" data-name=\"Layer 2\">\n",
       "\t\t<g id=\"Layer_1-2\" data-name=\"Layer 1\">\n",
       "\t\t\t<path style=\"fill:gray\" d=\"M22,32A21,21,0,1,1,1,53,21.024,21.024,0,0,1,22,32m0-1A22,22,0,1,0,44,53,22,22,0,0,0,22,31Z\"/>\n",
       "\t\t\t<line id=\"e2\" style=\"stroke:#f54040\" class=\"cls-1\" x1=\"87.5\" y1=\"53.003\" x2=\"130.5\" y2=\"53.003\"/>\n",
       "\t\t\t<line id=\"e2\" style=\"stroke:#f54040\" class=\"cls-1\" x1=\"150.344\" y1=\"53.003\" x2=\"193.344\" y2=\"53.003\"/>\n",
       "\t\t\t<path style=\"fill:gray\" class=\"t1\" d=\"M130,35.5v1a8,8,0,0,1,0,16v1a8.687,8.687,0,0,0,2.94-.5A9,9,0,0,0,130,35.5Z\"/>\n",
       "\t\t\t<path style=\"fill:gray\" class=\"t1\" d=\"M132.94,53a8.687,8.687,0,0,0-2.94-.5v1a8,8,0,0,1,0,16v1A9,9,0,0,0,132.94,53Z\"/>\n",
       "\t\t\t<path style=\"fill:gray\" class=\"t1\" d=\"M151.578,70.5v-1a8,8,0,0,1,0-16v-1a8.687,8.687,0,0,0-2.94.5,9,9,0,0,0,2.94,17.5Z\"/>\n",
       "\t\t\t<path style=\"fill:gray\" class=\"t1\" d=\"M148.638,53a8.687,8.687,0,0,0,2.94.5v-1a8,8,0,0,1,0-16v-1a9,9,0,0,0-2.94,17.5Z\"/>\n",
       "\t\t\t<line id=\"e4\" style=\"stroke:gray\" class=\"cls-1\" x1=\"194.5\" y1=\"38.503\" x2=\"253.5\" y2=\"38.503\"/>\n",
       "\t\t\t<polyline id=\"e7\" style=\"stroke:gray\" class=\"cls-2\" points=\"194.5 69.503 210.5 69.503 210.5 113.503 193.5 113.503\"/>\n",
       "\t\t\t<polyline id=\"e10\" style=\"stroke:gray\" class=\"cls-2\" points=\"194.625 145.097 210.625 145.097 210.625 189.097 193.625 189.097\"/>\n",
       "\t\t\t<line id=\"e5\" style=\"stroke:gray\" class=\"cls-1\" x1=\"253.313\" y1=\"38.699\" x2=\"312.313\" y2=\"38.699\"/>\n",
       "\t\t\t<line id=\"e6\" style=\"stroke:gray\" class=\"cls-1\" x1=\"313.219\" y1=\"38.25\" x2=\"372.219\" y2=\"38.25\"/>\n",
       "\t\t\t<line id=\"e8\" style=\"stroke:gray\" class=\"cls-1\" x1=\"193.641\" y1=\"130.22\" x2=\"252.641\" y2=\"130.22\"/>\n",
       "\t\t\t<line id=\"e9\" style=\"stroke:gray\" class=\"cls-1\" x1=\"252.453\" y1=\"130.416\" x2=\"311.453\" y2=\"130.416\"/>\n",
       "\t\t\t<line id=\"e11\" style=\"stroke:gray\" class=\"cls-1\" x1=\"194.734\" y1=\"205.611\" x2=\"253.734\" y2=\"205.611\"/>\n",
       "\t\t\t<polyline id=\"e12\" style=\"stroke:#303030\" class=\"cls-2\" points=\"312.5 114.503 298.5 114.503 298.5 54.503 313.5 54.503\"/>\n",
       "\t\t\t<line id=\"e1\" style=\"stroke:gray\" class=\"cls-1\" x1=\"43\" y1=\"53.003\" x2=\"86\" y2=\"53.003\"/>\n",
       "\t\t\t<polyline id=\"e3\" style=\"stroke:gray\" class=\"cls-2\" points=\"85.313 113.55 69.313 113.55 69.313 69.55 86.313 69.55\"/>\n",
       "\t\t\t<line class=\"cls-1\" x1=\"255\" y1=\"205.611\" x2=\"272.798\" y2=\"205.611\"/>\n",
       "\t\t\t<path style=\"fill:gray\" d=\"M265.963,210.411a.5.5,0,0,1,.154-.691l6.452-4.109-6.452-4.11a.5.5,0,1,1,.537-.843l7.115,4.531a.5.5,0,0,1,0,.844l-7.115,4.531a.5.5,0,0,1-.268.078A.5.5,0,0,1,265.963,210.411Z\"/>\n",
       "\t\t\t<line class=\"cls-1\" x1=\"315\" y1=\"23.503\" x2=\"332.798\" y2=\"23.503\"/>\n",
       "\t\t\t<path style=\"fill:gray\" d=\"M325.963,28.3a.5.5,0,0,1,.154-.69l6.452-4.11-6.452-4.109a.5.5,0,1,1,.537-.844l7.115,4.532a.5.5,0,0,1,0,.843l-7.115,4.532a.5.5,0,0,1-.691-.154Z\"/>\n",
       "\t\t\t<line class=\"cls-1\" x1=\"313\" y1=\"130.503\" x2=\"330.798\" y2=\"130.503\"/>\n",
       "\t\t\t<path style=\"fill:gray\" d=\"M323.963,135.3a.5.5,0,0,1,.154-.69l6.452-4.11-6.452-4.109a.5.5,0,1,1,.537-.844l7.115,4.532a.5.5,0,0,1,0,.843l-7.115,4.532a.5.5,0,0,1-.691-.154Z\"/>\n",
       "\t\t\t<line class=\"cls-1\" x1=\"313\" y1=\"114.503\" x2=\"330.798\" y2=\"114.503\"/>\n",
       "\t\t\t<path style=\"fill:gray\" d=\"M323.963,119.3a.5.5,0,0,1,.154-.69l6.452-4.11-6.452-4.109a.5.5,0,1,1,.537-.844l7.115,4.532a.5.5,0,0,1,0,.843l-7.115,4.532a.5.5,0,0,1-.691-.154Z\"/>\n",
       "\t\t\t<line class=\"cls-1\" x1=\"313\" y1=\"147.503\" x2=\"330.798\" y2=\"147.503\"/>\n",
       "\t\t\t<path style=\"fill:gray\" d=\"M323.963,152.3a.5.5,0,0,1,.154-.69l6.452-4.11-6.452-4.109a.5.5,0,1,1,.537-.844l7.115,4.532a.5.5,0,0,1,0,.843l-7.115,4.532a.5.5,0,0,1-.691-.154Z\"/>\n",
       "\t\t\t<text class=\"cls-3\" id=\"tv1\" style=\"fill:white\" transform=\"translate(17.438 24.503)\">1</text>\n",
       "\t\t\t<text class=\"cls-3\" id=\"tv2\" style=\"fill:white\" transform=\"translate(82.712 25.532)\">2</text>\n",
       "\t\t\t<text class=\"cls-3\" id=\"tv3\" style=\"fill:#86fc53\" transform=\"translate(190.65 25.048)\">3</text>\n",
       "\t\t\t<text class=\"cls-3\" id=\"tv4\" style=\"fill:#86fc53\" transform=\"translate(251.587 11.095)\">4</text>\n",
       "\t\t\t<text class=\"cls-3\" id=\"tv5\" style=\"fill:#86fc53\" transform=\"translate(310.806 10.813)\">5</text>\n",
       "\t\t\t<text class=\"cls-3\" id=\"tv6\" style=\"fill:#86fc53\" transform=\"translate(370.15 10.298)\">6</text>\n",
       "\t\t\t<text class=\"cls-3\" id=\"tv7\" style=\"fill:white\" transform=\"translate(83.775 100.938)\">7</text>\n",
       "\t\t\t<text class=\"cls-3\" id=\"tv8\" style=\"fill:#86fc53\" transform=\"translate(191.025 101.063)\">8</text>\n",
       "\t\t\t<text class=\"cls-3\" id=\"tv9\" style=\"fill:#86fc53\" transform=\"translate(251.15 102.548)\">9</text>\n",
       "\t\t\t<text class=\"cls-3\" id=\"tv10\" style=\"fill:#86fc53\" transform=\"translate(305.15 103.548)\">10</text>\n",
       "\t\t\t<text class=\"cls-3\" id=\"tv11\" style=\"fill:#86fc53\" transform=\"translate(187.125 177.657)\">\n",
       "\t\t\t\t<tspan class=\"cls-4\">1</tspan>\n",
       "\t\t\t\t<tspan x=\"5.783\" y=\"0\">1</tspan>\n",
       "\t\t\t</text>\n",
       "\t\t\t<text class=\"cls-3\" id=\"tv12\" style=\"fill:#86fc53\" transform=\"translate(246.602 177.462)\">12</text>\n",
       "\t\t\t<text class=\"cls-5\" id=\"te1\" style=\"fill:gray\" transform=\"translate(59.306 49.813)\">1</text>\n",
       "\t\t\t<text class=\"cls-5\" id=\"te2\" style=\"fill:#f54040\" transform=\"translate(136.65 31.048)\">2</text>\n",
       "\t\t\t<text class=\"cls-5\" id=\"te3\" style=\"fill:gray\" transform=\"translate(57.962 97.438)\">3</text>\n",
       "\t\t\t<text class=\"cls-5\" id=\"te4\" style=\"fill:gray\" transform=\"translate(217.056 51.001)\">4</text>\n",
       "\t\t\t<text class=\"cls-5\" id=\"te5\" style=\"fill:gray\" transform=\"translate(279.15 51.548)\">5</text>\n",
       "\t\t\t<text class=\"cls-5\" id=\"te6\" style=\"fill:gray\" transform=\"translate(341.087 51.556)\">6</text>\n",
       "\t\t\t<text class=\"cls-5\" id=\"te8\" style=\"fill:gray\" transform=\"translate(219.603 143.274)\">8</text>\n",
       "\t\t\t<text class=\"cls-5\" id=\"te9\" style=\"fill:gray\" transform=\"translate(281.697 143.821)\">9</text>\n",
       "\t\t\t<text class=\"cls-5\" id=\"te11\" style=\"fill:gray\" transform=\"translate(219.65 219.048)\">\n",
       "\t\t\t\t<tspan class=\"cls-4\">1</tspan>\n",
       "\t\t\t\t<tspan x=\"5.783\" y=\"0\">1</tspan>\n",
       "\t\t\t</text>\n",
       "\t\t\t<text class=\"cls-5\" id=\"te7\" style=\"fill:gray\" transform=\"translate(215.15 97.048)\">7</text>\n",
       "\t\t\t<text class=\"cls-5\" id=\"te10\" style=\"fill:gray\" transform=\"translate(214.758 172.798)\">10</text>\n",
       "\t\t\t<text class=\"cls-5\" id=\"te12\" style=\"fill:#303030\" transform=\"translate(281.313 88.798)\">12</text>\n",
       "\t\t\t\n",
       "\t\t\t<rect id=\"v11\" style=\"fill:#86fc53\" x=\"192.125\" y=\"182.597\" width=\"4\" height=\"45\"/>\n",
       "\t\t\t<rect id=\"v3\" style=\"fill:#86fc53\" x=\"192\" y=\"31.003\" width=\"4\" height=\"45\"/>\n",
       "\t\t\t<rect id=\"v4\" style=\"fill:#86fc53\" x=\"253\" y=\"16.003\" width=\"4\" height=\"45\"/>\n",
       "\t\t\t<rect id=\"v2\" style=\"fill:white\" x=\"84\" y=\"31.003\" width=\"4\" height=\"45\"/>\n",
       "\t\t\t<rect id=\"v8\" style=\"fill:#86fc53\" x=\"192\" y=\"107.003\" width=\"4\" height=\"45\"/>\n",
       "\t\t\t<rect id=\"v5\" style=\"fill:#86fc53\" x=\"311.813\" y=\"16.199\" width=\"4\" height=\"45\"/>\n",
       "\t\t\t<rect id=\"v9\" style=\"fill:#86fc53\" x=\"252.141\" y=\"107.72\" width=\"4\" height=\"45\"/>\n",
       "\t\t\t<rect id=\"v6\" style=\"fill:#86fc53\" x=\"371.719\" y=\"15.75\" width=\"4\" height=\"45\"/>\n",
       "\t\t\t<rect id=\"v7\" style=\"fill:white\" x=\"84\" y=\"106.003\" width=\"4\" height=\"45\"/>\n",
       "\t\t\t<rect id=\"v12\" style=\"fill:#86fc53\" x=\"253.234\" y=\"183.111\" width=\"4\" height=\"45\"/>\n",
       "\t\t\t<rect id=\"v10\" style=\"fill:#86fc53\" x=\"310.953\" y=\"107.916\" width=\"4\" height=\"45\"/>\n",
       "\t\t</g>\n",
       "\t</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vertex_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "edge_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "edge_vertex_id_pairs = [\n",
    "    (1, 2), (2, 3), (2, 7), (3, 4), (4, 5), (5, 6), (3, 8), (8, 9), (9, 10), (8, 11), (11, 12), (5, 10)\n",
    "]\n",
    "edge_enabled = [True, True, True, True, True, True, True, True, True, True, True, False]\n",
    "source_vertex_id = 1\n",
    "\n",
    "gp = pss.GraphProcessor(\n",
    "    vertex_ids, edge_ids, edge_vertex_id_pairs, edge_enabled, source_vertex_id\n",
    ")\n",
    "\n",
    "visualizer_init(gp)\n",
    "\n",
    "visualizer_downstream_vertices(gp, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726e14b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ec9d6",
   "metadata": {},
   "source": [
    "# **Optimal tap position**\n",
    "\n",
    "This section will detail the implementation of the optimal tap position search as well as the functionality implemented in assignment 2 for lower-level usage of the Power Grid Model package.\n",
    "\n",
    "First, all the necessary data has to be imported. The functions take in the already-read data as an input, so it has to loaded before the creation the the class. The necessary data includes:\n",
    "- The network description file, containing information on buses, lines, transformers, loads, and sources (.json PGM format)\n",
    "- The active profile batch update set for symmetrical loads (.parquet)\n",
    "- The reactive profile batch update set for symmetrical loads (.parquet)\n",
    "- The active profile batch update data set for EV charging, containing various example 'profiles' of how an EV charger would behave in a power grid (.parquet)\n",
    "- The meta data file (.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "403158c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_meta_data_json = \"presentation_A3/meta_data.json\"\n",
    "dir_network_json = \"presentation_A3/input_network_data.json\"\n",
    "dir_active_profile = \"presentation_A3/active_power_profile.parquet\"\n",
    "dir_reactive_profile = \"presentation_A3/reactive_power_profile.parquet\"\n",
    "dir_ev_active_profile =  \"presentation_A3/ev_active_power_profile.parquet\"\n",
    "\n",
    "with open(dir_meta_data_json) as fp:\n",
    "    data = fp.read()\n",
    "meta_data = json.loads(data)\n",
    "\n",
    "with open(dir_network_json) as fp:\n",
    "    data = fp.read()\n",
    "network_data = json_deserialize(data)\n",
    "\n",
    "active_profile = pd.read_parquet(dir_active_profile)\n",
    "reactive_profile = pd.read_parquet(dir_reactive_profile)\n",
    "ev_active_profile = pd.read_parquet(dir_ev_active_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f541a838",
   "metadata": {},
   "source": [
    "Afterwards, the LV Grid object is instantiated with the data. This is the only constructor of this class, and it performs all the necesasry checks for the corectedness of the data, which include graph description validity and update profile validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c75b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = psso.LvGrid(network_data, active_profile, reactive_profile, ev_active_profile, meta_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2c3cab",
   "metadata": {},
   "source": [
    "The optimal tap position function can then be used directly. This method requires the user to specify either 'energy_loss' or 'voltage_deviation' as the evaluation criteria, which are the total energy loss of all lines over the whole period and the deviation of p.u. node voltages with respect to 1 p.u, respectively. In either case, the goal is to minimize the quantity.\n",
    "\n",
    "The method returns a tuple, which contains the **node_id** to connect, and the calculated **quantity**.\n",
    "\n",
    "*If a different criterion is provided, the result will be (-1, -1).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "473f39b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, tuple[0.0750863213670021, 'min_tap'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.optimal_tap_position(\"voltage_deviation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffefcadc",
   "metadata": {},
   "source": [
    "## Optimal tap position - process\n",
    "\n",
    "The optimal tap position calculation can be rather slow. The general steps to find the optimal position for the given criteria are:\n",
    "1. Set the \"to\" connection of the transformer to a valid tap position X\n",
    "2. Change the Power Grid Model input to reflect the change of transformer connection\n",
    "3. Create a **PgmProcessor** object.\n",
    "4. Create the update models for the PgmProcessor object using **PgmProcessor.create_update_model()**\n",
    "5. Run the batch power flow using **PgmProcessor.run_batch_process()**\n",
    "6. Obtain the aggregated result tables from the calculation using **PgmProcessor.get_aggregate_results()**\n",
    "7. Compare the desired quantity in the table to the best recorded value - update the best recorded value to the new one if it is better.\n",
    "8. Repeat steps 1-7 for all other valid tap positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba961a2",
   "metadata": {},
   "source": [
    "### <u> PgmProcessor </u> \n",
    "\n",
    "The PgmProcessor class handles functionality related directly to the Power Grid Model package. In contrast, the graph processor handles fundamental graph-related methods, while LV_Grid is a high-level class for specific calculationns.\n",
    "\n",
    "Similar to the constructor for the LV grid, the necessary data consists of the .json network description, active profile, and reactive profile. A validity check is performed again on the input data (It is redundant if used from within optimization, but this class can also be used standalone).\n",
    "\n",
    "```python\n",
    "processor = pgm_p.PgmProcessor(\n",
    "    pgm_input, self.active_load_profile, self.reactive_load_profile\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c4dd7",
   "metadata": {},
   "source": [
    "### <u> PgmProcessor.create_update_model() </u> \n",
    "\n",
    "This method is a wrapper around the creation of batch update data sets for power flow analysis. After some (possibly redundant) validity cheks are performed, the method creates them as such:\n",
    "1. Create an *update array*. This update array is 3-dimensional and can be thought of as such:\n",
    " - Dimension 0 -> Each timestamp.\n",
    " - Dimension 1 -> (For each timestamp) Each symmetrical load.\n",
    " - Dimension 2 -> (For each timestamp and each symmetrical load) The instantenous active and reactive power at the node.\n",
    "2. Fill the update array with appropriate data, according to the active and reactive update profiles\n",
    "3. Create a *mutation dictionary*. This dictionary tells Power Grid Model that there is an *update array* for *sym_loads*.\n",
    "4. Validate the batch mutation data.\n",
    "\n",
    "```python\n",
    "def create_update_model(\n",
    "    self, use_active_load_profile: bool = None, use_reactive_load_profile: bool = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Validates update data and creates the time-series batch mutations of active and reactive load profiles of network nodes for power-flow analysis.\n",
    "    Args:\n",
    "        use_active_load_profile: if specificed, a different active load profile can be used than the one the class was initialized with.\n",
    "        use_reactive_load_profile: if specificed, a different active load profile can be used than the one the class was initialized with.\n",
    "    Raises:\n",
    "        ProfilesDontMatchError(0): if time series of both active and reactive profiles don't match\n",
    "        ProfilesDontMatchError(1): if node IDs are not the same in either profile\n",
    "        ProfilesDontMatchError(2): if node IDs are not the same in either profile, and in the Power Grid Model .json network description\n",
    "    \"\"\"\n",
    "    use_active_load_profile = use_active_load_profile or self.active_load_profile\n",
    "    use_reactive_load_profile = use_reactive_load_profile or self.reactive_load_profile\n",
    "\n",
    "    # Check if time series of both active and reactive profile match\n",
    "    if not use_active_load_profile.index.equals(use_reactive_load_profile.index):\n",
    "        raise ProfilesDontMatchError(0)\n",
    "\n",
    "    # Check if node IDs match in both profiles\n",
    "    if not use_active_load_profile.columns.equals(use_reactive_load_profile.columns):\n",
    "        raise ProfilesDontMatchError(1)\n",
    "\n",
    "    # Check if node IDs in both profiles match the node IDs in the PGM JSON input descriptor\n",
    "    if not np.array_equal(\n",
    "        pd.DataFrame(self.pgm_input[\"sym_load\"]).loc[:, \"id\"].to_numpy(),\n",
    "        use_active_load_profile.columns.to_numpy(),\n",
    "    ):\n",
    "        raise ProfilesDontMatchError(2)\n",
    "\n",
    "    # Validated, take any\n",
    "    self.update_index_length = use_active_load_profile.index.shape[0]\n",
    "    self.update_ids = use_active_load_profile.columns.to_numpy()\n",
    "\n",
    "    self.update_load_profile = pgm.initialize_array(\n",
    "        \"update\", \"sym_load\", (self.update_index_length, self.update_ids.shape[0])\n",
    "    )\n",
    "    self.update_load_profile[\"id\"] = self.update_ids\n",
    "    self.update_load_profile[\"p_specified\"] = use_active_load_profile\n",
    "    self.update_load_profile[\"q_specified\"] = use_reactive_load_profile\n",
    "    self.update_load_profile[\"status\"] = 1\n",
    "\n",
    "    self.time_series_mutation = {\"sym_load\": self.update_load_profile}\n",
    "    pgm.validation.assert_valid_batch_data(\n",
    "        input_data=self.pgm_input,\n",
    "        update_data=self.time_series_mutation,\n",
    "        calculation_type=pgm.CalculationType.power_flow,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf49fc16",
   "metadata": {},
   "source": [
    "### <u> PgmProcessor.run_batch_process() </u> \n",
    "\n",
    "This simple method runs the batch power flow calculation on the network with the previously-created mutation dictionary.\n",
    "\n",
    "```python\n",
    "def run_batch_process(self) -> None:\n",
    "    \"\"\"\n",
    "    Run the batch process on the input data according to the load update profile. Results are stored in a class variable.\n",
    "    \"\"\"\n",
    "    self.output_data = self.pgm_model.calculate_power_flow(\n",
    "        update_data=self.time_series_mutation,\n",
    "        calculation_method=CalculationMethod.newton_raphson,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c56fd9",
   "metadata": {},
   "source": [
    "### <u> PgmProcessor.get_aggregate_results() </u> \n",
    "\n",
    "This method returns the two aggregate tables, formatted to Pandas Dataframes and merged into a list. These are:\n",
    "- (Index 0 in return list): A table with each row representing a timestamp, with the following columns:\n",
    "    - Timestamp (index column)\n",
    "    - Maximum p.u. voltage of all the nodes for this timestamp\n",
    "    - The node ID with the maximum p.u. voltage\n",
    "    - Minimum p.u. voltage of all the nodes for this timestamp\n",
    "    - The node ID with the minimum p.u. voltage\n",
    "\n",
    "- (Index 1 in return list): A table with each row representing a line, with the following columns:\n",
    "    - Line ID (index column)\n",
    "    - Energy loss of the line across the timeline in kWh \n",
    "    - Maximum loading in p.u. of the line across the whole timeline\n",
    "    - Timestamp of this maximum loading moment\n",
    "    - Minimum loading in p.u. of the line across the whole timeline\n",
    "    - Timestamp of this minimum loading moment\n",
    "\n",
    "This data is extracted from the `self.output_data` calculated in the previous method. The total power loss is calculated using a trapezoidal integral.\n",
    "\n",
    "```python\n",
    "def get_aggregate_results(self) -> list[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generate the two required output tables based on the variables created in run_batch_process.\n",
    "    Returns:\n",
    "        2-element list of data frames for the 2 required aggregated tables\n",
    "    \"\"\"\n",
    "    # Initialize output variable\n",
    "    aggregate_results = [None, None]\n",
    "\n",
    "    # Make a list of all timestamps for later use\n",
    "    list_of_timestamps = self.active_load_profile.index.strftime(\"%Y-%m-%d %H:%M:%S\").to_list()\n",
    "\n",
    "    # Output dataframe for the first required table\n",
    "    df_min_max_nodes = pd.DataFrame()\n",
    "\n",
    "    # Loop through all timestamps, and pick the nodes with minimum and maximum voltage\n",
    "    for index, snapshot in enumerate(self.output_data[\"node\"]):\n",
    "\n",
    "        # Temporary dataframe which contains the timestamp snapshot data\n",
    "        df = pd.DataFrame(snapshot)\n",
    "\n",
    "        # Find the index of the row with the minimum and maximum value in the 'u_pu' column\n",
    "        max_index = df[\"u_pu\"].idxmax()\n",
    "        min_index = df[\"u_pu\"].idxmin()\n",
    "\n",
    "        # Retrieve the row with the minimum and maximum value in the 'u_pu' column\n",
    "        max_row = df.loc[max_index]\n",
    "        min_row = df.loc[min_index]\n",
    "\n",
    "        # Put the data in the correct rows, and columns\n",
    "        df_min_max_nodes.loc[list_of_timestamps[index], \"Max_Voltage_Node\"] = max_row[\"id\"]\n",
    "        df_min_max_nodes.loc[list_of_timestamps[index], \"Max_Voltage\"] = max_row[\"u_pu\"]\n",
    "        df_min_max_nodes.loc[list_of_timestamps[index], \"Min_Voltage_Node\"] = min_row[\"id\"]\n",
    "        df_min_max_nodes.loc[list_of_timestamps[index], \"Min_Voltage\"] = min_row[\"u_pu\"]\n",
    "\n",
    "    aggregate_results[0] = df_min_max_nodes\n",
    "\n",
    "    # Make a list with all the timestamp snapshots, to be used to find the max and min\n",
    "    flattened_list = [tuple for sublist in self.output_data[\"line\"] for tuple in sublist]\n",
    "    data = [\n",
    "        {\"id\": tpl[0], \"energized\": tpl[1], \"loading\": tpl[2], \"p_from\": tpl[3], \"p_to\": tpl[7]}\n",
    "        for tpl in flattened_list\n",
    "    ]\n",
    "\n",
    "    # Output dataframe for the second required output table\n",
    "    df_line_loss = pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Number of unique lines\n",
    "    num_nodes = df[\"id\"].nunique()\n",
    "    repeated_list_of_timestamps = [\n",
    "        elem for elem in list_of_timestamps for _ in range(num_nodes)\n",
    "    ]\n",
    "\n",
    "    # Add timestamp column to dataframe\n",
    "    df[\"Timestamp\"] = repeated_list_of_timestamps\n",
    "\n",
    "    # Group data by each line and then loop over each dataframe 'group'\n",
    "    grouped_by_line = df.groupby(\"id\")\n",
    "\n",
    "    for line_id, line in grouped_by_line:\n",
    "        line[\"p_loss\"] = abs(abs(line[\"p_from\"]) - abs(line[\"p_to\"]))\n",
    "\n",
    "        # Calculate the area under the power loss curve\n",
    "        energy_loss = integrate.trapezoid(line[\"p_loss\"].to_list()) / 1000\n",
    "\n",
    "        # Find the index of the row with the minimum and maximum value in the 'u_pu' column\n",
    "        max_index = line[\"loading\"].idxmax()\n",
    "        min_index = line[\"loading\"].idxmin()\n",
    "\n",
    "        # Retrieve the row with the minimum and maximum value in the 'u_pu' column\n",
    "        max_row = line.loc[max_index]\n",
    "        min_row = line.loc[min_index]\n",
    "\n",
    "        # Put the data in the correct rows, and columns\n",
    "        df_line_loss.loc[line_id, \"Total_Loss\"] = energy_loss\n",
    "        df_line_loss.loc[line_id, \"Max_Loading_Timestamp\"] = max_row[\"Timestamp\"]\n",
    "        df_line_loss.loc[line_id, \"Max_Loading\"] = max_row[\"loading\"]\n",
    "        df_line_loss.loc[line_id, \"Min_Loading_Timestamp\"] = min_row[\"Timestamp\"]\n",
    "        df_line_loss.loc[line_id, \"Min_Loading\"] = min_row[\"loading\"]\n",
    "\n",
    "    aggregate_results[1] = df_line_loss\n",
    "    aggregate_results[1].index.name = \"Line_ID\"\n",
    "    aggregate_results[0].index = pd.to_datetime(aggregate_results[0].index)\n",
    "    aggregate_results[0].index.name = \"Timestamp\"\n",
    "\n",
    "    return aggregate_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3445652e",
   "metadata": {},
   "source": [
    "## * PgmProcessor - validation\n",
    "\n",
    "### * <u> PgmProcessor.compare_to_expected(...) </u>\n",
    "\n",
    "This method is not used for the optimal tap position or EV charging, but it is used to verify that the aggregate tables are equal to what is expected. Example usage - as no errors occur, the calculated output is equal to the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42159e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dir_network_json = \"presentation_A2/input_network_data.json\"\n",
    "dir_active_profile = \"presentation_A2/active_power_profile.parquet\"\n",
    "dir_reactive_profile = \"presentation_A2/reactive_power_profile.parquet\"\n",
    "\n",
    "with open(dir_network_json) as fp:\n",
    "    data = fp.read()\n",
    "network_data = json_deserialize(data)\n",
    "\n",
    "active_profile = pd.read_parquet(dir_active_profile)\n",
    "reactive_profile = pd.read_parquet(dir_reactive_profile)\n",
    "\n",
    "# Run data and get results\n",
    "p = pgm_p.PgmProcessor(network_data, active_profile, reactive_profile)\n",
    "p.create_update_model()\n",
    "p.run_batch_process()\n",
    "aggregate_results = p.get_aggregate_results()\n",
    "\n",
    "# Save aggregate result\n",
    "aggregate_results[0].to_parquet(\"presentation_A2/calculated_output_per_timestamp.parquet\")\n",
    "aggregate_results[1].to_parquet(\"presentation_A2/calculated_output_per_line.parquet\")\n",
    "\n",
    "\n",
    "# Test if pre-calculated output data matches the aggregated output\n",
    "dir_out_per_line = \"presentation_A2/output_table_row_per_line.parquet\"\n",
    "dir_out_per_timestamp = \"presentation_A2/output_table_row_per_timestamp.parquet\"\n",
    "\n",
    "assert (\n",
    "    p.compare_to_expected(\n",
    "        aggregate_results,\n",
    "        pd.read_parquet(dir_out_per_line),\n",
    "        pd.read_parquet(dir_out_per_timestamp),\n",
    "    )\n",
    "    == True\n",
    ")\n",
    "\n",
    "# Change the output data and check if the error is detected\n",
    "with pytest.raises(AssertionError) as excinfo:\n",
    "    assert (\n",
    "        p.compare_to_expected(\n",
    "            aggregate_results,\n",
    "            pd.read_parquet(dir_out_per_line),\n",
    "            pd.read_parquet(dir_out_per_line),\n",
    "        )\n",
    "        == True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958cf33d",
   "metadata": {},
   "source": [
    "## Optimal tap position - visualization\n",
    "\n",
    "Both the aggregated data and raw output data from the batch power flow analysis can be visualized. The following options are available, with more being possible and easy to implement as needed due to the flexibility of the created framework:\n",
    "1. Animated\n",
    "    - Power flow - shows the **loading** of lines and symmetrical loads, as well as node **voltages** at each time stamp over time\n",
    "    - Load records - shows the highest and lowest symmetrical load **voltages** at each time stamp over time - similar to the general power flow one, but easier to pinpoint locations of interest\n",
    "    *(animations are exported in .gif format and opened externally)*\n",
    "    *(the number of frames can be specified, or frames=PgmProcessor.draw_aggregate_table[(0 or 1)].index.size can be used)*\n",
    "2. Static\n",
    "    - Line loading - shows either the **total loss**, **maximum loading**, or **minimum loading** for each line at each time stamp over time\n",
    "    - Node active power - plots the active power at the specified node over time in a line plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_meta_data_json = \"presentation_A3/meta_data.json\"\n",
    "dir_network_json = \"presentation_A3/input_network_data.json\"\n",
    "dir_active_profile = \"presentation_A3/active_power_profile.parquet\"\n",
    "dir_reactive_profile = \"presentation_A3/reactive_power_profile.parquet\"\n",
    "dir_ev_active_profile =  \"presentation_A3/ev_active_power_profile.parquet\"\n",
    "\n",
    "with open(dir_meta_data_json) as fp:\n",
    "    data = fp.read()\n",
    "meta_data = json.loads(data)\n",
    "\n",
    "with open(dir_network_json) as fp:\n",
    "    data = fp.read()\n",
    "network_data = json_deserialize(data)\n",
    "\n",
    "active_profile = pd.read_parquet(dir_active_profile)\n",
    "reactive_profile = pd.read_parquet(dir_reactive_profile)\n",
    "ev_active_profile = pd.read_parquet(dir_ev_active_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6820d41",
   "metadata": {},
   "source": [
    "A) **General power flow for a network (limited to 10 frames)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb0aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pgm_p.PgmProcessor(network_data, active_profile, reactive_profile)\n",
    "p.create_update_model()\n",
    "p.run_batch_process()\n",
    "aggregate_results = p.get_aggregate_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "fig.subplots_adjust(left=0.1, right=0.7, bottom=0.1, top=0.9)\n",
    "\n",
    "p.draw_to_networkx(aggregate_results, ax)\n",
    "p.draw_init_power_flow('u_pu')\n",
    "\n",
    "animation = matplotlib.animation.FuncAnimation(fig, p.draw_timelapse_power_flow, init_func=p.draw_init, frames=10, interval=250, blit=False)\n",
    "animation.save(\"vis_A.gif\", writer='Pillow', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd3119d",
   "metadata": {},
   "source": [
    "B) **Aggregate per-load results from EV penetration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c34c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "lv = psso.LV_grid(network_data, active_profile, reactive_profile, ev_active_profile, meta_data)\n",
    "aggregate_results = lv.EV_penetration_level(0.8, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb11de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "fig.subplots_adjust(left=0.1, right=0.55, bottom=0.1, top=0.9)\n",
    "\n",
    "p.draw_to_networkx(aggregate_results, ax)\n",
    "\n",
    "animation = matplotlib.animation.FuncAnimation(fig, p.draw_timelapse_node_loading, init_func=p.draw_init, frames=10, interval=250, blit=False)\n",
    "animation.save(\"vis_B.gif\", writer='Pillow', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5f1ef1",
   "metadata": {},
   "source": [
    "C) **Static line-loading**\n",
    "This method uses the aggregate data from draw_to_networkx(aggregate_results, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c61f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_results = p.get_aggregate_results()\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "fig.subplots_adjust(left=0.1, right=0.55, bottom=0.1, top=0.9)\n",
    "\n",
    "p.draw_to_networkx(aggregate_results, ax)\n",
    "p.draw_static_line_loading('power_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b7c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_results = p.get_aggregate_results()\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "fig.subplots_adjust(left=0.1, right=0.55, bottom=0.1, top=0.9)\n",
    "\n",
    "p.draw_to_networkx(aggregate_results, ax)\n",
    "p.draw_static_line_loading('max_loading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c10daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_results = p.get_aggregate_results()\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "fig.subplots_adjust(left=0.1, right=0.55, bottom=0.1, top=0.9)\n",
    "\n",
    "p.draw_to_networkx(aggregate_results, ax)\n",
    "p.draw_static_line_loading('min_loading')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b34503",
   "metadata": {},
   "source": [
    "D) **Drawing a single EV load quantity (for example - and what is used - active power on node)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e833347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_results = lv.EV_penetration_level(0.8, False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "fig.subplots_adjust(left=0.1, right=0.55, bottom=0.1, top=0.9)\n",
    "\n",
    "lv.processor.draw_static_simple_load_active(lv.processor.output_data, list(lv.House_Profile_Id.keys())[0], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd5b19",
   "metadata": {},
   "source": [
    "# **N-1 Calculation: Alternative grid topology**\n",
    "\n",
    "In this function an edge is disconnected. The goal is to find a table where different alternative edges can be compared. \n",
    "For this, a conversion of the input data is required. This way, the functions made in Assignment 1 can be used:\n",
    "```python \n",
    "vertex_ids = [node[0] for node in self.pgm_input[\"node\"]]\n",
    "edge_ids = [edge[0] for edge in self.pgm_input[\"line\"]]\n",
    "edge_vertex_id_pairs = [(edge[1], edge[2]) for edge in self.pgm_input[\"line\"]]\n",
    "edge_enabled = [(edge[3] == 1 and edge[4] == 1) for edge in self.pgm_input[\"line\"]]\n",
    "source_vertex_id = self.pgm_input[\"source\"][0][1]\n",
    "``` \n",
    "In this assignment we also have transformers. For the purpose of this function, these will be considered as normal edges and will be added to the edge_ids list:\n",
    "```python \n",
    "for transformer in self.pgm_input[\"transformer\"]:\n",
    "    edge_ids.append(transformer)\n",
    "```\n",
    "# Using the graph processor\n",
    "Now that we have converted the given input data, we can initialize a graph processor object:\n",
    "```python \n",
    "gp = pss.GraphProcessor(\n",
    "    vertex_ids, edge_ids, edge_vertex_id_pairs, edge_enabled, source_vertex_id\n",
    ")\n",
    "```\n",
    "Using the object, we are now able to call: \n",
    "```python \n",
    "gp.find_alternative_edges(line_id)\n",
    "```\n",
    "The function above was written for Assignment 1.\n",
    "For now we know that this function returns a list of all alternative edges. A more in depth explanation will be given shortly.\n",
    "A constraint of the power system is that: \n",
    "```python\n",
    "\"6. The graph should be fully connected.\"\n",
    "```\n",
    "If we would disable an edge, the function looks for all alternative edges it can enable to have the system be fully connected again, thus following the mentioned contraint of the system. \n",
    "The function returns a list. For each alternative edge in that list, N-1 calculation will do as the name suggest, calculations. This is done using functions created in assignment 2. Since these have already been explained. A better way to finish this explanation is to tell what we do with the output of those functions.\n",
    "We give each alternative edge a row with:\n",
    "- The alternative Line ID to be connected\n",
    "- The maximum loading in Watt among all lines and timestamps\n",
    "- The Line ID of this maximum\n",
    "- The timestamp of this maximum\n",
    "\n",
    "Multiple alternative edges means that the table will consist of multiple rows. No alternative edges means that there are no rows in the table and thus having an empty one.\n",
    "\n",
    "## Alternative Edge Algorithm Steps\n",
    "\n",
    "- Get a list of all the disabled edges\n",
    "- Loop through the list of disabled edges, and one by one enable each disabled edge\n",
    "- Check if the graph is fully connected and non-circular\n",
    "- If it is, add it to the list of alternative edges\n",
    "\n",
    "Or in code form:\n",
    "```python\n",
    "alternative_edges = []\n",
    "disabled_edge_list = []\n",
    "\n",
    "edge_list.disable_edge(input_edge)\n",
    "\n",
    "for edge in edge_list:\n",
    "    if is_disabled(edge) is True:\n",
    "        disabled_edge_list.append(edge)\n",
    "\n",
    "for edge in disabled_edge_list:\n",
    "    # Temporarily enable edge\n",
    "    edge_list.enable_edge(edge)\n",
    "\n",
    "    if not graph.isCyclic() and graph.isFullyConnected():\n",
    "        alternative_edges.append(edge)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cab629-eced-4033-84d2-e110bbc447bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer_init(gp)\n",
    "\n",
    "visualizer_alternative_edge(gp, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba3d9f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2716d4eb",
   "metadata": {},
   "source": [
    "# **Testing**\n",
    "\n",
    "Testing the functions was done seperately between the assignments. In the very first assignment the downstream vertices and alternative edges functions are tested, also including some important errors. \n",
    "\n",
    "What basically has been done is that certain test cases are written in which the code should raise an error. For example when we check if all edges are unique, we make a case where this is not valid:\n",
    "\n",
    "```python\n",
    "# Non-unique edge-ID\n",
    "t_edge_ids = [1, 3, 5, 7, 8, 8]\n",
    "with pytest.raises(pss.IDNotUniqueError, match=r\".*T1\"):\n",
    "    pss.GraphProcessor(\n",
    "        vertex_ids, t_edge_ids, edge_vertex_id_pairs, edge_enabled, source_vertex_id\n",
    "    )\n",
    "```\n",
    "And the same was done for all the other errors, but also for the two functions. These two functions are done differently however; \n",
    "We create graphs and compare the functions output with the intended output of what the functions are supposed to give. For example a case for alternative edges is tested like this:\n",
    "\n",
    "```python\n",
    "def test_alternative_edges():\n",
    "    # Graph being made for the testing of this function \n",
    "    vertex_ids = [0, 2, 4, 6, 10]\n",
    "    edge_ids = [1, 3, 5, 7, 8, 9]\n",
    "    edge_vertex_id_pairs = [(0, 2), (0, 4), (0, 6), (2, 4), (4, 6), (2, 10)]\n",
    "    edge_enabled = [True, True, True, False, False, True]\n",
    "    source_vertex_id = 0\n",
    "\n",
    "    gp = pss.GraphProcessor(\n",
    "        vertex_ids, edge_ids, edge_vertex_id_pairs, edge_enabled, source_vertex_id\n",
    "    )\n",
    "    # When the functions is called, the alternative edge it should\n",
    "    # give is edge 7 when edge 1 is being disabled\n",
    "    assert set(gp.find_alternative_edges(1)) == set([7])\n",
    "```\n",
    "The graph that is being made looks like this:\n",
    "```python\n",
    "# vertex_0 (source) --edge_1(enabled)-- vertex_2 --edge_9(enabled)-- vertex_10\n",
    "#                  |                               |\n",
    "#                  |                           edge_7(disabled)\n",
    "#                  |                               |\n",
    "#                  -----------edge_3(enabled)-- vertex_4\n",
    "#                  |                               |\n",
    "#                  |                           edge_8(disabled)\n",
    "#                  |                               |\n",
    "#                  -----------edge_5(enabled)-- vertex_6\n",
    "```\n",
    "When the intended results match the results that is provided by our function reconfirms that the functions are working as intended.\n",
    "\n",
    "For assignment 2 the same thing was done with the errors, but the functions are different. What now is done is that the results of the tables are being compared with the provided results of the given data. Here is and example of an error where there is a mismatch in timestamps:\n",
    "```python\n",
    "# Change a timestamp in active profile to an incorrect one and check for error\n",
    "active_load_profile_wrong = active_load_profile.copy()\n",
    "active_load_profile_wrong.rename(\n",
    "    index={active_load_profile.index[0]: pd.to_datetime(\"today\").normalize()}, inplace=True\n",
    ")\n",
    "with pytest.raises(pgm_p.ProfilesDontMatchError, match=r\".*T0\") as excinfo:\n",
    "    p = pgm_p.PgmProcessor(network_data, active_load_profile_wrong, reactive_load_profile)\n",
    "    p.create_update_model()\n",
    "```\n",
    "Below is also something similar as assignment 1, but now the resulting tables are being compared to the provided tables of the data:\n",
    "\n",
    "```python\n",
    "assert (\n",
    "    p.compare_to_expected(\n",
    "        aggregate_results,\n",
    "        pd.read_parquet(dir_out_per_line),\n",
    "        pd.read_parquet(dir_out_per_timestamp),\n",
    "    )\n",
    "    == True\n",
    ")\n",
    "# Change the output data and check if the error is detected\n",
    "with pytest.raises(AssertionError) as excinfo:\n",
    "    assert (\n",
    "        p.compare_to_expected(\n",
    "            aggregate_results,\n",
    "            pd.read_parquet(dir_out_per_line),\n",
    "            pd.read_parquet(dir_out_per_line),\n",
    "        )\n",
    "        == True\n",
    "    )\n",
    "```\n",
    "\n",
    "The testing of assignment 3 is a combination of the already existing classes and functions. \n",
    "The input data is checked for multiple different types of errors to make sure that the data is valid.\n",
    "An example for this is the error check below:\n",
    "```python\n",
    "# 10 The number of EV charging profile is at least the same as the number of sym_load.\n",
    "test_number_EV = copy.deepcopy(ev_active_profile)\n",
    "new_column = copy.deepcopy(test_number_EV[1])\n",
    "test_number_EV[4] = new_column\n",
    "with pytest.raises(psso.EvProfilesDontMatchSymLoad) as excinfo:\n",
    "    psso.LV_grid(network_data, active_profile, reactive_profile, test_number_EV, meta_data)\n",
    "```\n",
    "It is practically done the same as in assignment 1 and 2, but here the input data is altered to where it should give an error.\n",
    "Furthermore, the testing of the functions are kind of already done in the previous assignments so it was not necessarily needed to test them again, except for the optimal tap postition. In the optimal tap position the calculated position is compared with the intended position to see whether the function is working as intended.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da11fd6",
   "metadata": {},
   "source": [
    "# **Conclusion**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86dd12a",
   "metadata": {},
   "source": [
    "## **Teamwork**\n",
    "\n",
    "Someone?: [Explain how we worked together, using github and regular meetings and stuff, maybe planning etc]\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50612008",
   "metadata": {},
   "source": [
    "### * <u> Drawing a large network </u>\n",
    "This illustrates how such a large network would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43d36de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_meta_data_json = \"D:/presentation_large/meta_data.json\"\n",
    "dir_network_json = \"D:/presentation_large/input_network_data.json\"\n",
    "dir_active_profile = \"D:/presentation_large/active_power_profile.parquet\"\n",
    "dir_reactive_profile = \"D:/presentation_large/reactive_power_profile.parquet\"\n",
    "dir_ev_active_profile =  \"D:/presentation_large/ev_active_power_profile.parquet\"\n",
    "\n",
    "with open(dir_meta_data_json) as fp:\n",
    "    data = fp.read()\n",
    "meta_data = json.loads(data)\n",
    "\n",
    "with open(dir_network_json) as fp:\n",
    "    data = fp.read()\n",
    "network_data = json_deserialize(data)\n",
    "\n",
    "active_profile = pd.read_parquet(dir_active_profile)\n",
    "reactive_profile = pd.read_parquet(dir_reactive_profile)\n",
    "ev_active_profile = pd.read_parquet(dir_ev_active_profile)\n",
    "\n",
    "p = pgm_p.PgmProcessor(network_data, active_profile, reactive_profile)\n",
    "p.create_update_model()\n",
    "p.run_batch_process()\n",
    "aggregate_results = p.get_aggregate_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dae579",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "fig.subplots_adjust(left=0.1, right=0.7, bottom=0.1, top=0.9)\n",
    "\n",
    "p.draw_to_networkx(aggregate_results, ax)\n",
    "p.draw_init_power_flow('u_pu')\n",
    "\n",
    "animation = matplotlib.animation.FuncAnimation(fig, p.draw_timelapse_power_flow, init_func=p.draw_init, frames=10, interval=250, blit=False)\n",
    "animation.save(\"vis_large.gif\", writer='Pillow', dpi=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
